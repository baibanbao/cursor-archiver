
real廖海波
@realliaohaibo
第一性原理的思考方式和不依第一性原理的思考方式差别何在？我以为主要是两点：
1. 打破路径依赖;
2. 遍历解决方案。

第一性原理思考问题的时候，本质是类似欧氏几何，从少有的几个公理推导出整个理论体系。

由于不涉及现有实践，就脱离了现有路径依赖。

而脱离了路径依赖，尝试所有的可能的路径，就提供了遍历解决方案的可能。

群里的朋友提了一个很好的例子：常年以来中餐都被认为是不可以被标准化的。但如果从第一性原理出发，我们把中餐的所有组件分开，似乎没有哪一种常见食材和哪一个常用烹饪环节是不能标准化的。（罕见食材和那些罕见的手段西餐也一样不能标准化。）现在西贝，真功夫，海底捞等不同业态的中餐也证明了中餐标准化的可行性。

这和马斯克把火箭拆分之后，每一种材料和每一种加工都不那么贵，所以推出成本可以大大降低是一个道理。

第一性原理自下而上，把时间线上被遮掩的可能全部揭示出来了。

当然它只是一个方法论，有效应用第一性原理的前提是对于可能的解决方案要有足够的知识储备。

比如：一个从来没做过饭的人试图从底层拆解中餐标准化，它可能完全不知道加热时间对食物的影响，由此不可能成功。

fin
@fi56622380
大家都喜欢引用musk回收火箭的例子，我心目中first principle最佳则是费曼1959年谈到芯片尺寸极限在哪里，当年读书时看到这篇就是惊为天人的感觉

1959年费曼有一个著名的lecture叫There's Plenty of Room at the Bottom，在谈到电脑能做到多小的时候，直接从物理原理出发，说导线可以做到10个原子尺度那么大，  说我们一定会达到操纵原子的水平，得考虑量子隧穿效应以及散热

要知道当时的电脑还是晶体管时代，一个电脑有一两个衣柜那么大。在快速迭代的科技行业里预测十年都很困难，而他六十多年前就能看到半导体的极限大概在什么地方，而这些Vision在不久之后就成为了全人类觉得理所当然的常识

人类在四十多年之后才突破了他说的100nm大小的电脑，虽然他预见不到现在的半导体能做很多层而不是单层，想象不出现在的人类能在finfet上玩出花来还能玩出3D，但丝毫不影响这个论断给人带来的惊艳感 

First principle并不是万能，本质上是一种跳出局部最优找寻全局最优的方法

简单的说就是把尺度放大(特别是时间尺度)，自然会过滤掉很多高频/短期噪音，在更大尺度上更重要的因素自然就浮现出来了

在ML领域，跳出局部找全局最佳通常的思想就是引入随机性/noise，比如Simulated annealing/Genetic algorithms/Particle filter，其实本质上也是从全局上cold restart，从整体上重新审视

第一性原理什么时候最有用？

定大方向的时候，需要避免短期hype的情绪做判断的时候(没错，说的就是GPT/LLM)

预测上限的时候，比如信息领域里的香农极限，费曼的芯片尺寸极限

遇到互相矛盾的现象需要理解或者判断哪个是靠谱的时候


See new Tweets
Conversation
fin
@fi56622380
从第一性原理出发，我来尝试探讨一下GPT/LLM的准确率或者说靠谱性的极限在哪里

这个极限不是一个绝对值，而是一个关系：LLM的准确率和推理深度不可兼得，推理层数越多越容易错，可以称为越思考越错原理

类似海森堡测不准原理：不可能同时精确测量一个基本粒子的位置和速度

这个极限本质上取决于LLM的目标函数：语言模型需要预测下一个词出现的概率，每一个输出词都是根据之前已经产生的输出词来填空产出，目标函数需要最大化范本和输出词一样的概率

目标函数就是概率，那么每个词输出的时候，必然会有概率出错，我们假定这个出错概率是e，那么输出一段话如果有n个词，这段话不出错的概率就是(1-e)^n，说的越多错误概率指数型增长，这就是我们经常会发现GPT一本正经的胡说八道的原因

我们寄予厚望的LLM推理能力，本质上是一种统计推断：简单推理结果都被语言统计结果覆盖的时候(比如说只能向前推理一两步)，那么在绝大多数情况下，仅从输入输出结果的角度看，这个统计模型就等价于有了推理能力

在利用思考链chain of thought之后，也就是每一步往下推理一小步，对一个复杂的任务慢慢推理出一个逻辑链，那么就有了复杂逻辑推理的能力，面对复杂问题的正确率提高了非常多

但这种思考链的每一层同样也是有概率出错的，每一层出错概率就是前面提到的(1-e)^n，那么如果思考链有m层，随着思考深度的增加，出错的概率同样是指数增长：(1-e)^(m*n)

这就意味着LLM每一步思考都会增加出错的概率，即便是运用了RLHF(人工反馈增强学习)，本质上也就是减小这个e的值而已，在指数增长面前不值一提。而指数型增加出错概率是一个很可怕的限制，意味着随着思考深度的增加，结果就越来越不可信，意味着任何的推理，组合，如果不想看到误导性结果，都只能浅尝辄止

通俗的讲，就是思考越多越错，也可以叫越思考越错原理

如果只让LLM提供信息，或者简单的推理，那LLM出错的概率是能控制的很好的，但只要推理的步骤多了，那么出错的概率就会非常大

打个比方，GPT4目前的factorial事实准确率是不到90%，那么如果是一次性要求推理一个稍微长一点的逻辑链，准确率就会迅速的降到50%以下。即便是把逻辑步骤分拆，稍微长一点的逻辑链准确率也会迅速的降低到低水平(除非每一步让人校正)，毕竟指数型降低的威力是巨大的


LiUgOd
@LiuGods
·
Mar 31
Replying to 
@fi56622380
人类的推理思考是怎么保证准确率的呢
fin
@fi56622380
·
Mar 31
Replying to 
@LiuGods
好问题

个体的思考无法保证准确率，群体思考经过互相印证，方法论，无数的验证，以及借助工具实践的检验，才把这个错误率降低到了也许10的负10次方以下

但群体智慧仍然是有错误的时候，地球是平的，地心说等等

只是在现代借助了方法论和各种验证方法的严谨性之后，这个准确率才变得可靠起来

