Meta的人工智能大师LeCun： 今天大多数的人工智能方法都不会导致真正的智能。

LeCun说，许多深度学习的基本问题都无法解决，包括如何衡量信息的奥秘。

30分钟阅读查看原文

yann-lecun-sept-2022-1

Meta公司的首席人工智能科学家Yann LeCun说："我认为人工智能系统需要能够进行推理，"。今天流行的人工智能方法，如变形金刚，其中许多是建立在他自己在该领域的开创性工作之上，将是不够的。"你必须退一步说，好吧，我们建了这个梯子，但我们想去月球，这个梯子不可能让我们到达那里，"LeCun说。

(文章在上下文中更新了加里-马库斯和于尔根-施密特胡贝尔的反驳。)

Facebook、Instagram和WhatsApp的所有者Meta Properties的首席人工智能科学家Yann LeCun，可能会在他的领域里激怒很多人。

6月，LeCun在开放评论服务器上发表了一篇思想文章，对他认为有希望在机器中实现人类水平的智能的方法进行了广泛的概述。

论文中隐含的观点是，今天的大多数人工智能大项目将永远无法达到人类水平的目标，即使没有明确说明。

在本月通过Zoom与ZDNET的讨论中，LeCun明确表示，他对目前深度学习中许多最成功的研究途径抱有极大的怀疑态度。

这位图灵奖得主在谈到他的同行们的追求时说："我认为它们是必要的，但并不充分"。

这些包括大型语言模型，如基于变压器的GPT-3及其同类产品。正如LeCun所描述的那样，Transformer的信徒们相信："我们把一切都标记化，并训练巨大的模型来进行离散的预测，而人工智能将以某种方式从这里出现。

"他们没有错，"他说，"在这个意义上，这可能是未来智能系统的一个组成部分，但我认为它缺少基本的部分。"

还有： Meta的人工智能名人LeCun探讨了深度学习的能量边界

这是对看似有效的东西的一个惊人的批评，来自于完善使用卷积神经网络的学者，这种实用的技术在深度学习程序中取得了令人难以置信的成果。

LeCun在该学科其他大量非常成功的领域中看到了缺陷和限制。

他认为，强化学习也永远不够。DeepMind的大卫-西尔弗（David Silver）等研究人员开发了掌握国际象棋、将棋和围棋的AlphaZero程序，他们正专注于 "非常基于行动 "的程序，LeCun说，但 "我们所做的大部分学习，并不是通过实际行动来完成的，我们是通过观察来完成的。"

62岁的勒库恩，从几十年的成就来看，还是表达了一种紧迫感，要面对他认为许多人可能正在急于走向的盲道，并试图将他的领域哄骗到他认为事情应该发展的方向。

"他说："我们看到很多关于我们应该做什么来推动人类水平的人工智能的主张。"而有些想法我认为是错误的。"

"我们还没有达到我们的智能机器像猫一样有常识的地步，"Lecun说。"那么，我们为什么不从那里开始呢？"

他已经放弃了之前在预测视频中的下一帧等方面使用生成网络的信心。"他说："这已经是一个完全的失败。

LeCun谴责那些他称之为 "宗教概率论者 "的人，他们 "认为概率论是你可以用来解释机器学习的唯一框架"。

他说，纯粹的统计方法是难以解决的。"要求一个世界模型完全是概率性的，这要求太高了；我们不知道如何做到这一点。"

LeCun认为，不仅仅是学术界，工业界的人工智能也需要进行深入的重新思考。他说，自动驾驶汽车的人群、Wayve等初创公司已经 "有点太乐观了"，他们认为他们可以 "向 "大型神经网络 "扔数据"，"你可以学到几乎任何东西。"

"你知道，我认为我们完全有可能拥有没有常识的五级自动驾驶汽车，"他说，指的是自动驾驶的 "ADAS"，即高级驾驶辅助系统术语，"但你将不得不对它进行工程设计。"

他认为，这种过度工程化的自动驾驶技术将是一种像所有被深度学习所淘汰的计算机视觉程序一样吱吱作响、脆弱不堪的东西。

"最终，会有一个更令人满意的、可能是更好的解决方案，它涉及到在理解世界运作方式方面做得更好的系统。"

一路上，LeCun对他最大的批评者提出了一些尖锐的看法，比如纽约大学教授加里-马库斯--"他从未对人工智能做出过任何贡献"，以及达勒-莫勒人工智能研究所的联合主任于尔根-施密胡贝--"做插旗的事非常容易"。

除了批评之外，LeCun提出的更重要的一点是，某些基本问题面临着所有的人工智能，特别是如何衡量信息。

"你必须退一步说，好吧，我们建了这个梯子，但我们想去月球，这个梯子不可能把我们带到那里，"LeCun说他希望促使人们重新思考基本概念。"基本上，我在这里写的是，我们需要建造火箭，我不能告诉你我们如何建造火箭的细节，但这里是基本原则。"

通过阅读LeCun今年早些时候接受ZDNET的采访，可以更好地理解这篇论文，以及LeCun在采访中的想法，他在采访中主张将基于能量的自我监督学习作为深度学习的发展道路。这些反思让我们了解到他希望建立的核心方法，作为他声称无法达到终点的替代方案。

以下是经过轻度编辑的采访记录。

ZDNET： 我们聊天的主题是这篇论文《通往自主机器智能的道路》，其中0.9.2版本是现存版本，是吗？

Yann LeCun： 是的，我认为这算是一个工作文件。所以，我把它发布在开放评论上，等待人们提出意见和建议，也许还有更多的参考资料，然后我再制作一个修订版。

ZDNET： 我看到Juergen Schmidhuber已经在Open Review上发表了一些评论。

YL：嗯，是的，他总是这样。我在我的论文中引用了他的一篇论文。我认为他在社交网络上提出的论点，即他基本上是在1991年发明了所有这些，就像他在其他情况下所做的那样，这根本不是事实。我的意思是，这是很容易做flag-planting，而且，有点，写一个想法，没有任何实验，没有任何理论，只是建议你可以这样做。但是，你知道，仅仅有这个想法，然后让它在一个玩具问题上起作用，然后让它在一个真正的问题上起作用，然后做一个理论，说明它为什么起作用，然后部署它，这之间有很大的区别。这有一个完整的链条，而他对科学功劳的看法是，应该由第一个有这个想法的人得到所有的功劳，你知道。这是很荒谬的。

(更新：Jürgen Schmidhuber回答说："LeCun声称，我的'科学功劳的想法是，它是第一个人，只是，有点，你知道，有这个想法，应该得到所有的功劳。在任何情况下这都不是真的。正如我所写的[DLC]："一个重要方法的发明者应该获得发明的荣誉。她不一定是那个普及它的人。那么，推广者就应该为推广它（但不是为发明它）而获得荣誉。然而，LeCun显然并不满足于普及他人发明的功劳；他还想得到发明者的功劳。他正在加倍坚持一个站不住脚的立场，这与普遍接受的科学诚信的基本规则不相容[T22]。

ZDNET： 不要相信你在社交媒体上听到的一切。

YL：我是说，他说我应该引用的主要论文并没有我在论文中谈到的任何主要观点。他在GANs和其他事情上也是这样做的，结果并不是真的。做插旗的事很容易，要做出贡献就难多了。顺便说一下，在这篇特定的论文中，我明确表示这不是一篇通常意义上的科学论文。它更像是一份关于这个东西应该走向何方的立场文件。那里有几个想法可能是新的，但大部分不是。从本质上讲，我并不要求我在那份文件中写的大部分内容有任何优先权。

(更新：Schmidhuber回答说："LeCun声称我：'......他说我应该引用的主要论文并没有我在论文中谈到的任何主要观点。这毫无意义。我列出的不仅仅是一篇而是几篇相关的论文（包括[AC90][UN1][AC02][HRL1][PLAN4]），其中包含了LeCun明确称为他的'主要原创贡献'的大部分内容[LEC22a]。LeCun说[LEC22c]：'对于我在那篇论文中写的大部分内容，我基本上没有要求任何优先权。然而，他列出了他的'主要原创性贡献'[LEC22a]，而我表明它们不是[LEC]。LeCun声称我'他在GANs方面也做了这个'。这个错误的说法没有任何理由，也没有任何参考资料。我在1990年发表的基于梯度的生成性和对抗性NN[AC90-AC90b]描述了2014年GAN的（经常被引用并实施和使用的）基本原理--其论文[GAN1]未能正确分配信用[T22]。我在这方面的同行评议出版物[AC20]仍然没有受到质疑。)

LeCun认为，强化学习也永远不够。DeepMind的David Silver等研究人员开发了掌握国际象棋、将棋和围棋的AlphaZero程序，他们 "非常基于行动"，LeCun说，但 "我们所做的大部分学习，不是通过实际行动来完成的，而是通过观察来完成的。"

ZDNET： 这也许是一个很好的开始，因为我很好奇你现在为什么要追求这条道路？是什么让你想到了这个？你为什么想写这个？

YL：嗯，所以，我已经想了很久了，关于走向人类水平或动物水平的智能或学习和能力的道路。在我的演讲中，我一直在大声疾呼，监督学习和强化学习都不足以模仿我们在动物和人类身上观察到的那种学习。我这样做已经有七、八年了。所以，这并不是最近的事。许多年前，我在NeurIPS上做了一个主题演讲，我基本上提出了这个观点，还有各种讲座，都有录音。现在，为什么现在要写一篇论文？我已经到了这个地步--[谷歌大脑研究员]杰夫-辛顿（Geoff Hinton）也做过类似的事情--我的意思是，当然，他比我更清楚，我们看到时间在流逝。我们不年轻了。

ZDNET： 60岁是新的50岁。

YL：这是真的，但问题是，我们看到很多关于我们应该做什么来推动人类水平的人工智能的说法。而有些想法我认为是错误的。所以，一种想法是，哦，我们应该在神经网络之上添加符号推理。而我不知道如何做到这一点。所以，也许我在论文中所解释的可能是一种方法，它可以在没有明确符号操作的情况下做同样的事情。这就是传统意义上的世界上的加里-马库斯的那种做法。顺便说一下，加里-马库斯不是一个AI人，他是一个心理学家。他从未对人工智能做出任何贡献。他在实验心理学方面做了非常好的工作，但他从来没有写过一篇关于人工智能的同行评议的论文。所以，这就是那些人。

(更新：加里-马库斯反驳了缺乏同行评议文章的说法。他通过电子邮件向ZDNet提供了以下经过同行评议的文章： 使用《人工智能》中的不完全信息对容器进行常识性推理；从不完全信息中进行推理： 容器的例子》（Advances In Cog Sys）；《人工智能自动推理》（Automated Reasoning）中模拟的范围和限制；《ACM通讯》（Communications of the ACM）中的常识性推理和常识性知识；《反思消除性联结主义》（Cog Psy)

世界上有一个[DeepMind主要研究科学家]David Silvers，他说，你知道，奖励已经足够了，基本上，这都是关于强化学习的，我们只需要让它更有效率一点，好吗？我认为他们没有错，但我认为使强化学习更有效的必要步骤，基本上会把强化学习降为蛋糕上的樱桃。而主要缺失的部分是学习世界是如何运作的，主要是通过观察而不采取行动。强化学习是非常基于行动的，你通过采取行动和看到结果来学习关于这个世界的东西。

ZDNET： 而且它是以奖励为重点的。

YL：它是以奖励为重点的，也是以行动为重点的。所以，你必须在这个世界上采取行动，才能对这个世界有所了解。我在论文中提出的关于自我监督学习的主要主张是，我们所做的大部分学习，不是通过实际行动来进行的，而是通过观察来进行的。这是非常非正统的，特别是对强化学习的人来说，但对很多心理学家和认知科学家来说也是如此，他们认为，你知道，行动是--我不是说行动不是必要的，它是必要的。但我认为我们学习的大部分内容主要是关于世界的结构，并且涉及，当然，互动、行动和游戏，以及诸如此类的东西，但很多是观察性的。

ZDNET： 你也会同时设法勾掉变形金刚的人，即语言第一的人。没有语言，你怎么能建立这个？你可能会得罪很多人。

YL：是的，我已经习惯了。所以，是的，有一些语言第一的人，他们说，你知道，智力是关于语言的，智力的底层是语言，等等，等等，等等。但是，这有点否定了动物智力。你知道，我们的智能机器还没有达到像猫一样有常识的地步。那么，我们为什么不从那里开始呢？是什么让猫能够理解周围的世界，做相当聪明的事情，并计划和类似的东西，而狗甚至更好？

然后有很多人说，哦，智力是一种社会性的东西，对吗？我们之所以聪明，是因为我们互相交谈，交流信息，等等，等等。有各种从未见过父母的非社会性物种都非常聪明，比如章鱼或红毛猩猩。我的意思是，它们[红毛猩猩]当然会受到母亲的教育，但它们不是社会性动物。

但是，我可能会勾起的另一类人是那些说缩放已经足够的人。所以，基本上，我们只是使用巨大的变形金刚，我们在多模态数据上训练它们，包括，你知道，视频，文本，等等，等等。我们，有点，石化一切，并将一切标记化，然后训练巨大的模型来进行离散的预测，基本上，以某种方式，人工智能将出现在这里。他们没有错，在这个意义上，这可能是未来智能系统的一个组成部分。但我认为它缺少基本的部分。

在这篇论文中，还有一类人是我要勾掉的。那就是概率论者，宗教概率论者。所以，那些认为概率理论是你可以用来解释机器学习的唯一框架的人。正如我在文章中试图解释的那样，要求一个世界模型完全是概率论的，这基本上是太过分了。我们不知道如何做到这一点。还有就是计算上的不可行性。所以我提议放弃这整个想法。当然，你知道，这不仅是机器学习的一个巨大支柱，也是所有统计学的支柱，它声称是机器学习的正常形式主义。

另一件事--

ZDNET： 你是在做梦...

YL: - 就是所谓的生成模型。所以，你可以学习预测的想法，你也许可以通过预测了解到很多关于世界的东西。所以，我给你一段视频，我要求系统预测视频中接下来会发生什么。而且我可能会要求你预测实际的视频帧，包括所有的细节。但我在论文中所争论的是，这实际上要求太多，太复杂。这也是我改变主意的地方。直到大约两年前，我一直是所谓的潜变量生成模型的倡导者，即预测接下来会发生什么或缺少什么信息的模型，可能在潜变量的帮助下，如果预测不能是确定性的。而我已经放弃了这个。我放弃的原因是基于经验性的结果，人们曾试图应用BERT和大型语言模型中使用的那种预测或基于重建的训练，他们曾试图将其应用于图像，但完全失败。它完全失败的原因是，再次因为概率模型的限制，在那里预测像单词这样的离散标记相对容易，因为我们可以计算出字典中所有单词的概率分布。这很容易。但是，如果我们要求系统在所有可能的视频帧上产生概率分布，我们不知道如何将其参数化，或者我们知道如何将其参数化，但我们不知道如何将其规范化。它遇到了一个难以解决的数学问题，我们不知道如何解决。

"我们还没有达到我们的智能机器像猫一样有常识的地步，"莱库恩观察到。"那么，我们为什么不从那里开始呢？是什么让一只猫能够领会周围的世界，做相当聪明的事情，以及计划和类似的东西，而狗甚至更好？"

所以，这就是为什么我说让我们放弃概率论或类似东西的框架，较弱的那个，基于能量的模型。我一直在倡导这个，也有几十年了，所以这不是最近的事情。但与此同时，放弃生成模型的想法，因为世界上有很多事情是无法理解的，无法预测的。如果你是一个工程师，你把它叫做噪音。如果你是一个物理学家，你叫它热。如果你是一个机器学习的人，你把它叫做，你知道，不相关的细节或其他。

因此，我在论文中使用的例子，或者我在会谈中使用的例子，是，你想要一个世界预测系统，这将有助于自动驾驶汽车，对吗？它希望能够提前预测所有其他汽车的轨迹，预测其他可能移动的物体会发生什么，行人、自行车、追着足球跑的孩子，诸如此类的事情。所以，关于这个世界的各种事情都有。但是，在路边，可能有树，今天有风，所以树叶在风中移动，在树后面有一个池塘，池塘里有波纹。而这些，从本质上说，基本上是不可预测的现象。而且，你不希望你的模型花费大量的资源来预测那些既难以预测又不相关的东西。因此，这就是为什么我提倡联合嵌入架构，那些你试图建模的变量，你不是试图预测它，而是试图对它进行建模，但它通过一个编码器运行，而这个编码器可以消除很多关于输入的细节，这些细节是不相关的或太复杂的--基本上，相当于噪音。

ZDNET： 我们在今年早些时候讨论了基于能量的模型，即JEPA和H-JEPA。我的感觉是，如果我对你的理解正确的话，你在寻找低能量的点，这两个预测的X和Y嵌入是最相似的，这意味着如果在一个树上有一只鸽子，在一个场景的背景里有一些东西，这些可能不是使这些嵌入彼此接近的基本点。

YL: 对。因此，JEPA架构实际上试图找到一个折衷办法，在提取对输入有最大信息量的表征和对彼此有某种程度的准确性或可靠性的预测之间进行折衷。它找到了一个权衡。因此，如果它可以选择花费大量的资源包括树叶运动的细节，然后对决定一秒钟后树叶如何运动的动态进行建模，或者只是通过消除所有这些细节的预测器来运行Y变量，从而将其丢在地上，它可能只是消除它，因为它只是太难建模和捕捉。

ZDNET： 令人惊讶的是，你曾经是一个伟大的支持者，说 "它是有效的，我们以后会想出热力学理论来解释它"。在这里，你采取的方法是，"我不知道我们如何一定要解决这个问题，但我想提出一些想法来思考它，"也许甚至接近一个理论或假设，至少。这很有意思，因为有很多人花了很多钱在研究能看到行人的汽车，而不管汽车是否有常识。而我想象这些人中的一些人将会，不是生气，而是说，"这很好，我们不在乎它是否有常识，我们已经建立了一个模拟，这个模拟是惊人的，我们将继续改进，我们将继续扩大模拟。"

因此，有趣的是，你现在有资格说，让我们退一步，想想我们在做什么。而业界正在说我们只是要扩大、扩大、扩大、扩大，因为那个曲柄真的很有效。我的意思是，GPU的半导体曲柄真的很有效。

YL：这里有五个问题。所以，我的意思是，扩大规模是必要的。我不是在批评我们应该扩大规模的事实。我们应该扩大规模。那些神经网络越大越好。毫无疑问，我们应该扩大规模。而那些具有某种程度的常识的神经网络将是很大的。我想这是没有办法的事。因此，扩大规模是好的，它是必要的，但不是充分的。这就是我的观点。它不仅仅是缩放。这是第一点。

第二点，理论是否是第一位的，以及诸如此类的事情。所以，我认为有些概念是先有的，你必须退一步说，好吧，我们建了这个梯子，但我们想去月球，这个梯子不可能带我们去。所以，基本上，我在这里写的是，我们需要建造火箭。我不能告诉你我们如何建造火箭的细节，但这里是基本原则。而且我不是在为它写理论或什么，但是，它将是一个火箭，好吗？或一个太空电梯或任何东西。我们可能没有所有技术的所有细节。我们正在努力使其中一些东西发挥作用，比如我一直在研究JEPA。联合嵌入在图像识别方面效果非常好，但要用它来训练一个世界模型，有困难。我们正在努力，我们希望我们很快就能使它工作，但我们可能会在那里遇到一些障碍，我们无法克服，可能。

然后，论文中有一个关于推理的关键想法，如果我们希望系统能够计划，你可以认为是推理的一种简单形式，它们需要有潜在的变量。换句话说，那些不是由任何神经网络计算出来的东西，而是那些--其价值被推断出来以便最小化一些目标函数，一些成本函数。然后你可以使用这个成本函数来驱动系统的行为。这根本不是一个新的想法，对吗？这是非常经典的最优控制，其基础可以追溯到50年代末、60年代初。所以，在这里并不声称有什么新意。但我想说的是，这种类型的推理必须是智能系统的一部分，它能够进行规划，其行为可以被指定或控制，不是通过硬接线行为，不是通过模仿靠拢，而是通过一个目标函数来驱动行为--不一定驱动学习，但它驱动行为。你知道，我们的大脑中就有这样的东西，每一种动物都有内在的成本或内在的事情动机。这促使九个月大的婴儿想要站起来。当你站起来的时候，快乐的成本，成本函数中的那个项是硬性规定的。但你如何站起来却不是，那是学习。

"扩展是好的，它是必要的，但不是充分的，"LeCun说，巨型语言模型，如基于Transformer的GPT-3种类的程序。Transformer的信徒们认为，"我们把一切都标记化，训练巨大的模型来进行离散的预测，不知不觉中人工智能就会从中出现......但我认为它缺少基本的部分。"

ZDNET： 只是为了圆这个观点，深度学习社区的大部分人似乎对没有常识的东西没有意见，一意孤行。看起来你在这里提出了一个相当明确的论点，即在某些时候它变成了一个僵局。有些人说，我们不需要有常识的自动驾驶汽车，因为缩放会做到这一点。听起来你好像在说继续沿着这条路走下去是不行的？

YL：你知道，我认为我们完全有可能拥有没有常识的五级自动驾驶汽车。但这种方法的问题是，这将是暂时的，因为你将不得不对它进行工程设计。所以，你知道，绘制整个世界的地图，硬连接各种特定的角落行为，收集足够的数据，你有所有你在道路上可能遇到的奇怪情况，等等，等等。我的猜测是，只要有足够的投资和时间，你就可以把它设计出来。但最终，会有一个更令人满意和可能更好的解决方案，涉及的系统能更好地理解世界的运作方式，并具有，你知道，某种程度上我们称之为常识的东西。它不需要是人类水平的常识，但系统可以通过观察获得某种类型的知识，但不是看人开车，只是看东西移动，了解世界的很多情况，建立一个关于世界如何运作的背景知识基础，在此基础上你可以学习开车。

让我举一个历史上的例子来说明这一点。经典的计算机视觉是基于大量的硬接线、工程模块，在此基础上，你会有一种，薄薄的学习层。因此，在2012年被AlexNet击败的东西，基本上有一个第一阶段，有点像手工制作的特征提取，像SIFT[尺度不变特征变换（SIFT），一种经典的视觉技术，用于识别图像中的突出对象]和HOG[定向梯度直方图，另一种经典技术]和其他各种东西。然后，第二层是基于特征核等的中间层特征，以及某种无监督的方法。然后在这上面，你放一个支持向量机，或者其他一个相对简单的分类器。这是从2000年代中期到2012年的标准管道。然后被端到端的卷积网所取代，在那里你不需要硬连接任何东西，你只有大量的数据，你从头到尾训练这个东西，这是我长期以来一直提倡的方法，但是你知道，直到那时，对大型问题来说是不实用的。

在语音识别方面也有类似的故事，同样，对于如何预处理数据，提取大规模的倒频谱[用于信号处理的快速傅里叶变换的倒数]，然后你有隐马尔科夫模型，有某种预先设定的架构，等等，等等，有高斯混合体。因此，这有点像视觉的架构，你有手工制作的前端，然后是一个有点无监督的、训练有素的中间层，然后是一个监督层在上面。而现在，这基本上已经被端到端的神经网络所消灭了。因此，我看到了类似的东西，试图学习一切，但你必须有正确的先验，正确的架构，正确的结构。

他说，自动驾驶汽车的人群，如Waymo和Wayve等初创公司，已经 "有点太乐观了"，他们认为他们可以 "向它扔数据，你可以学到几乎任何东西"。ADAS第五级的自动驾驶汽车是可能的，"但你必须对它进行工程设计"，其结果会像早期的计算机视觉模型一样 "很脆"。

ZDNET： 你的意思是，有些人将试图把目前深度学习不适用的东西工程化，比如说，在工业领域，他们将开始创造一些在计算机视觉中变得过时的东西？

YL：对。这也是过去几年从事自动驾驶工作的人有点过于乐观的部分原因，因为你知道，你有这些，有点像卷积网和变形金刚的通用东西，你可以把数据扔给它，它可以学习几乎任何东西。所以，你说，好吧，我有办法解决这个问题。你做的第一件事是建立一个演示，让汽车自己开几分钟，不伤害任何人。然后你意识到有很多角落里的情况，你试着绘制曲线，当我把训练集增加一倍时，我得到了多大的改善，你意识到你永远不会到达那里，因为有各种各样的角落情况。你需要有一辆车，每2亿公里不到就会发生一次致命的事故，对吗？那么，你是怎么做的？好吧，你向两个方向走。

第一个方向是，我怎样才能减少我的系统学习所需的数据量？而这就是自我监督学习的地方。因此，很多自动驾驶汽车公司对自我监督学习非常感兴趣，因为这是一种仍然使用巨大的监督数据进行模仿学习的方法，但通过预训练，基本上可以获得更好的性能。它还没有完全实现，但它会实现的。然后还有另一种选择，在这一点上，大多数比较先进的公司都采用了这种方法，那就是，好吧，我们可以做端到端的训练，但有很多角落的情况我们无法处理，所以我们将只是设计系统来处理这些角落的情况，而且，基本上，把它们当作特殊情况，硬连接控制，然后硬连接很多基本行为来处理特殊情况。如果你有一个足够大的工程师团队，你可能会把它完成。但这需要很长的时间，而且到最后，它仍然会有点脆，也许足够可靠，你可以部署，但有一定程度的脆性，随着未来可能出现的更多基于学习的方法，汽车将不会有，因为它可能有某种程度的常识和对世界如何运作的理解。

在短期内，某种程度上，工程化的方法将获胜--它已经获胜。这就是世界上的Waymo和Cruise以及Wayveand什么的，这就是他们的工作。然后是自我监督学习方法，这可能会帮助工程化方法取得进展。但是，从长远来看，对那些公司来说，这可能是太长的等待，可能是，一种更综合的自主智能驾驶系统。

ZDNET： 我们说超出大多数投资者的投资范围。

YL：是的。所以，问题是，在业绩达到预期水平之前，人们会不会失去耐心或耗尽资金。

ZDNET： 关于你为什么在模型中选择了一些元素，有没有什么有趣的说法？因为你引用了Kenneth Craik [1943,The Nature of Explanation]，你还引用了Bryson和Ho [1969,Applied optimal control]，我很好奇你为什么从这些影响开始，如果你特别相信这些人已经把它钉在了他们的工作上。你为什么从那里开始？

YL：嗯，我不认为，当然，他们有所有的细节钉。所以，Bryson和Ho，这是我在1987年读的一本书，当时我在多伦多和Geoffrey Hinton做博士后。但是我在写博士论文的时候就已经知道这一行了，并且从本质上把最优控制和反推法联系起来。如果你真的想成为，你知道，另一个Schmidhuber，你会说Backprop的真正发明者实际上是最优控制理论家Henry J. Kelley、Arthur Bryson，甚至可能是Lev Pontryagin，他是50年代后期的俄罗斯最优控制理论家。

所以，他们想通了，事实上，你实际上可以看到这个的根源，这下面的数学，是拉格朗日力学。因此，你可以回到欧拉和拉格朗日，事实上，在他们对拉格朗日经典力学的定义中，可以找到一丝这方面的信息，真的。因此，在优化控制的背景下，这些人感兴趣的基本上是计算火箭的轨迹。你知道，这是早期的太空时代。如果你有一个火箭的模型，它告诉你这里是火箭在时间t的状态，这里是我要采取的行动，所以，推力和各种执行器，这里是火箭在时间t+1的状态。

ZDNET： 一个状态动作模型，一个价值模型。

YL：没错，是控制的基础。所以，现在你可以通过想象一连串的命令来模拟火箭的射击，然后你有一些成本函数，也就是火箭到目标的距离，一个空间站或任何东西。然后通过某种梯度下降，你可以计算出，我怎样才能更新我的行动序列，使我的火箭实际上尽可能地接近目标。而这必须通过时间上的反向传播信号来实现。这就是反向传播，梯度反向传播。这些信号，在拉格朗日力学中被称为共轭变量，但事实上，它们是梯度。所以，他们发明了反向传播，但他们没有意识到这个原理可以用来训练一个多阶段的系统，可以做模式识别或类似的事情。这一点直到70年代末、80年代初才真正意识到，然后直到80年代中期才真正实现并发挥作用。好的，所以，这就是Backprop真正起飞的地方，因为人们展示了几行代码，你可以训练一个神经网，从头到尾，多层。这解除了Perceptron的限制。而且，是的，它与最佳控制有联系，但这也没关系。

ZDNET： 所以，这是一个很长的说法，你开始时的这些影响是回到了Backprop，这对你来说是很重要的一个起点？

YL：是的，但我认为人们有点忘记了，在这方面有相当多的工作，你知道，早在90年代，甚至80年代，包括像迈克尔-乔丹[麻省理工学院大脑和认知科学系]这样的人，他们现在不做神经网络了，但你可以使用神经网络进行控制，你可以使用经典的最优控制思想。因此，像所谓的模型预测控制，现在被称为模型预测控制，这个想法是，如果你有一个你试图控制的系统和它所处环境的良好模型，你可以模拟或想象一连串行动的结果。然后通过梯度下降，基本上--这不是学习，这是推理--你可以找出能使我的目标最小的最佳行动序列。因此，我认为，使用带有潜在变量的成本函数进行推理，是目前大规模神经网络的作物所遗忘的东西。但在很长一段时间里，它是机器学习的一个非常经典的组成部分。因此，每一个贝叶斯网或图形模型或概率图形模型都使用这种类型的推理。你有一个捕捉一堆变量之间的依赖关系的模型，你被告知一些变量的值，然后你必须推断出其余变量的最可能的值。这就是图形模型和贝叶斯网以及诸如此类的推理的基本原则。而且我认为这基本上就是推理应该有的内容，推理和规划。

ZDNET： 你是一个封闭的贝叶斯主义者。

YL：我是一个非概率贝叶斯主义者。我以前开过这个玩笑。实际上，几年前我在NeurIPS，我想是在2018年或2019年，我被一个贝叶斯学家拍到了，他问我是否是一个贝叶斯学家，我说，是的，我是一个贝叶斯学家，但我是一个非概率贝叶斯学家，算是，一个基于能量的贝叶斯学家，如果你想。

ZDNET： 这听起来绝对像是《星际迷航》里的东西。你在这篇论文的结尾提到，要实现你的设想，需要多年的艰苦工作。告诉我目前的一些工作包括什么。

YL: 所以，我在论文中解释了你是如何训练和建立JEPA的。我主张的标准是要有某种方式来最大限度地提高所提取的表征对输入的信息含量。然后，第二个标准是使预测误差最小化。如果你在预测器中有一个潜在的变量，使预测器成为非确定性的，你必须通过最小化其信息含量来规范这个潜在的变量。因此，你现在有两个问题，一个是你如何最大化某个神经网络输出的信息含量，另一个是你如何最小化某个潜变量的信息含量？而如果你不做这两件事，系统就会崩溃。它将不会学到任何有趣的东西。它将给一切事物以零能量，类似这样的东西，这不是一个好的依赖性模型。这就是我提到的预防崩溃的问题。

而我说在人们曾经做过的所有事情中，只有两类方法可以防止崩溃。一种是对比性方法，另一种是那些规范化的方法。所以，这种将两个输入的表征的信息含量最大化、将潜变量的信息含量最小化的想法，属于正则化方法。但是那些联合嵌入架构中的很多工作都在使用对比性方法。事实上，它们可能是目前最流行的。因此，问题是你究竟如何以一种可以优化或最小化的方式来衡量信息内容？这就是事情变得复杂的地方，因为我们实际上不知道如何测量信息含量。我们可以对它进行近似测量，我们可以对它进行上限测量，我们可以做这样的事情。但他们并没有真正衡量信息含量，实际上，在某种程度上，信息含量甚至没有得到很好的定义。

ZDNET： 这不是香农定律？这不是信息理论？你有一定的熵，好的熵和坏的熵，好的熵是一个能工作的符号系统，坏的熵是噪音。这不都被香农解决了吗？

YL：你是对的，但这背后有一个重大缺陷。你是对的，如果你有数据向你袭来，并且你能以某种方式将数据量化为离散的符号，然后你测量每个符号的概率，那么这些符号所携带的最大信息量是可能的符号之和Pi log Pi，对吗？其中Pi是符号i的概率--这就是香农熵。[香农定律通常被表述为H = - ∑ pi log pi]。

不过，问题来了： Pi是什么？当符号的数量较少，并且符号是独立绘制的时候，这很容易。当有很多符号，并且有依赖关系时，就很难了。所以，如果你有一个比特序列，并且你假设这些比特是相互独立的，并且概率在1和0之间相等或其他，那么你可以很容易地测量熵，没有问题。但如果出现在你面前的是高维向量，比如，你知道，视频帧，或类似的东西，Pi是什么？分布是什么？首先你必须对这个空间进行量化，这是一个高维的、连续的空间。你不知道如何正确地量化这个。你可以使用K-means，等等。这就是人们在做视频压缩和图像压缩时的做法。但这只是一个近似值。然后你必须做出独立性的假设。所以，很明显，在一个视频中，连续的帧不是独立的。有依赖性，这一帧可能取决于你一小时前看到的另一帧，那是同一事物的图片。所以，你知道，你不能测量Pi。为了测量Pi，你必须有一个机器学习系统来学习预测。于是你又回到了之前的问题。所以，你只能从本质上对信息的测量进行近似。

"问题是你究竟如何以一种可以优化或最小化的方式来衡量信息含量？"LeCun说。"而这正是事情变得复杂的地方，因为我们实际上不知道如何衡量信息含量。" 到目前为止，能做的最好的事情就是找到一个 "对我们想要的任务来说足够好 "的代理。

让我举一个更具体的例子。我们一直在玩的一个算法，我在文章中也谈到了，就是这个叫VICReg的东西，方差-方差-正则化。这是在ICLR上发表的一篇单独的论文，大约在一年前被放在arXiv上，2021年。那里的想法是使信息最大化。这个想法实际上来自于我的小组的一篇早期论文，叫做Barlow Twins。你通过假设变量之间的唯一依赖关系是相关的，即线性依赖关系，来最大化神经网络中的向量的信息含量。因此，如果你假设成对的变量之间，或你的系统中的变量之间，唯一可能的依赖关系是成对的贵重物品之间的相关性，这是极其粗略的近似，那么你可以通过确保所有的变量具有非零方差--比方说，方差一，它是什么并不重要--来最大化你的系统出来的信息含量，然后对它们进行反向关联，同样的过程被称为美白，它也不是新的。这样做的问题是，你完全可以在变量组或甚至只是一对变量之间有极其复杂的依赖关系，而这些依赖关系不是线性的，它们不会显示在相关性中。因此，例如，如果你有两个变量，而这两个变量的所有点都以某种螺旋状排列，这两个变量之间就有非常强的依赖关系，对吗？但事实上，如果你计算这两个变量之间的相关性，它们是不相关的。所以，这里有一个例子，这两个变量的信息含量实际上非常小，它只有一个量，因为这是你在螺旋中的位置。它们是去相关的，所以你认为你有很多信息从这两个变量中出来，而事实上你没有，你只有，你知道，你可以从另一个变量中预测一个变量，本质上。所以，这表明我们只有非常近似的方法来衡量信息含量。

ZDNET： 所以这也是你现在必须要研究的事情之一，有了这个？这是一个更大的问题，即我们如何知道我们什么时候在最大化和最小化信息含量？

YL：或者说，我们在这方面使用的代理是否足够好，可以完成我们想要的任务。事实上，我们在机器学习中一直在这样做。我们最小化的成本函数从来不是我们真正想要最小化的。所以，比如说，你想做分类，好吗？当你训练一个分类器时，你想最小化的成本函数是分类器所犯的错误数量。但这是一个不可分的，可怕的成本函数，你不能最小化，因为，你知道，你要改变你的神经网络的权重，什么都不会改变，直到其中一个样本翻转了它的决定，然后在错误中出现跳跃，正或负。

ZDNET： 所以你有一个代理，这是一个目标函数，你可以肯定地说，我们肯定可以流动这个东西的梯度。

YL：是这样的。所以人们使用这个交叉熵损失，或SOFTMAX，你有几个名字，但它是同样的事情。它基本上是对系统所犯错误数量的平滑近似，其中平滑是通过，基本上，考虑到系统给每个类别的分数来完成的。

ZDNET： 有什么我们没有涉及到的、你想涉及的内容吗？

YL：可能是强调要点。我认为人工智能系统需要能够进行推理，而我所倡导的这个过程是使一些目标相对于一些潜在的变量最小化。这允许系统进行规划和推理。我认为我们应该放弃概率框架，因为当我们想做一些事情时，比如捕捉高维、连续变量之间的依赖关系，它是难以做到的。而我主张放弃生成模型，因为系统将不得不投入太多资源来预测那些太难预测的事情，也许会消耗太多的资源。而这差不多就是了。这就是主要的信息，如果你想的话。然后是整体架构。然后是那些关于意识的性质和配置器的作用的猜测，但这确实是猜测。

ZDNET： 我们下次再来讨论这个问题。我本来想问你，你是怎么给这个东西做基准的？但我猜你现在离基准测试还有点远？

YL：不一定那么远，算是简化版。你可以做每个人在控制或强化学习中所做的事情，也就是，你训练这个东西去玩雅达利游戏或类似的东西，或其他一些有一些不确定性的游戏。

ZDNET： 谢谢你的时间，Yann。